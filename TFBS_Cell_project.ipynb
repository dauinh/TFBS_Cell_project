{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "u-psvXpKG_x8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4D9OdA0JYgif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996a4231-d7ea-4925-8e27-2c26f66caae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e: print(e)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_DIR=\"/content/drive/MyDrive/Shared/TFBS_Cell_project/\"\n",
        "in_path=ROOT_DIR+\"cellvar.db.tfbs_seq.tsv.gz\"\n",
        "out_dir=ROOT_DIR+\"out\"\n",
        "\n",
        "if path.exists(out_dir) == False:\n",
        "    os.mkdir(out_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NFwoVcfHYx7",
        "outputId": "79a53737-7e7b-44ce-d1b6-9ce4df4c5814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comma-seperated list fo features to model: AEC,PEC,PC,IC\n",
        "features=\"AEC\"\n",
        "\n",
        "# split factor [0.0 to 1.0]\n",
        "split=\"0.5\"\n",
        "\n",
        "# maximum vocabulary size\n",
        "max_vocab=5\n",
        "\n",
        "# sequence length\n",
        "maxlen=50\n",
        "\n",
        "# balance factor [0.0 is downsample to 1.0 is upsample]\n",
        "balance_w=0.5\n",
        "\n",
        "# gpu number [0 to x]\n",
        "gpu_num=1"
      ],
      "metadata": {
        "id": "58aFffPZIVLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EiRPOh24YoMS"
      },
      "outputs": [],
      "source": [
        "# des = \"\"\"\n",
        "# TFBS sequence training/learning tool\n",
        "# (C) Timothy James Becker, 05/03/24-07/08/24, version=0.0.1\"\"\"\n",
        "# parser = argparse.ArgumentParser(description=des, formatter_class=argparse.RawTextHelpFormatter)\n",
        "# # need the split, max vocab, maxlen, balance, cmap,tsv.gz input file\n",
        "# parser.add_argument('--in_path', type=str, help='cellvar.db.tfbs_seq.tsv.gz traing input file')\n",
        "# parser.add_argument('--out_dir', type=str, help='output directory')\n",
        "# parser.add_argument('--features', type=str, help='comma-seperated list fo features to model: AEC,PEC,PC,IC')\n",
        "# parser.add_argument('--split', type=float, help='split factor [0.0 to 1.0]')\n",
        "# parser.add_argument('--vocab', type=int, help='maximum vocabulary size')\n",
        "# parser.add_argument('--len', type=int, help='sequence length')\n",
        "# parser.add_argument('--balance', type=float, help='balance factor [0.0 is downsample to 1.0 is upsample]')\n",
        "# parser.add_argument('--gpu', type=int, help='gpu number [0 to x]')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# in_path = args.in_path\n",
        "# features = args.features.split(',')\n",
        "# features = set([feature.upper() for feature in features]).intersection(set(['AEC', 'PEC', 'PC', 'IC']))\n",
        "# split = args.split\n",
        "# max_vocab = args.vocab\n",
        "# maxlen = args.len\n",
        "# balance_w = args.balance\n",
        "# gpu_num = args.gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network layers"
      ],
      "metadata": {
        "id": "9YRp0xKLHDAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NHNmYBMAYy48"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim,self.num_heads,self.ff_dim,self.rate = embed_dim, num_heads, ff_dim, rate\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.maxlen,self.vocab_size,self.embed_dim = maxlen, vocab_size, embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z_yeyVpJY5Qa"
      },
      "outputs": [],
      "source": [
        "class F1(keras.metrics.Metric):\n",
        "    def __init__(self, name='f1'):\n",
        "        super().__init__(name=name)\n",
        "        self.sum_t0   = self.add_variable(shape=(),initializer='zeros',name='sum_t0')\n",
        "        self.sum_p0   = self.add_variable(shape=(),initializer='zeros',name='sum_p0')\n",
        "        self.sum_t0p0 = self.add_variable(shape=(),initializer='zeros',name='sum_t0p0')\n",
        "        self.sum_t1   = self.add_variable(shape=(),initializer='zeros',name='sum_t1')\n",
        "        self.sum_p1   = self.add_variable(shape=(),initializer='zeros',name='sum_p1')\n",
        "        self.sum_t1p1 = self.add_variable(shape=(),initializer='zeros',name='sum_t1p1')\n",
        "        self.f1       = self.add_variable(shape=(),initializer='zeros',name='f1')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        t0 = -1 * (y_true - 1) + 0.0\n",
        "        t1 = y_true\n",
        "        p0 = ops.round(y_pred[:, 0],0)\n",
        "        p1 = ops.round(y_pred[:, 1],0)\n",
        "        self.sum_t0.assign(self.sum_t0+ops.sum(t0))\n",
        "        self.sum_p0.assign(self.sum_p0+ops.sum(p0))\n",
        "        self.sum_t0p0.assign(self.sum_t0p0+ops.sum(t0*p0))\n",
        "        self.sum_t1.assign( self.sum_t1+ops.sum(t1))\n",
        "        self.sum_p1.assign(self.sum_p1+ops.sum(p1))\n",
        "        self.sum_t1p1.assign(self.sum_t1p1+ops.sum(t1*p1))\n",
        "\n",
        "    def result(self):\n",
        "        prec0 = ops.divide_no_nan(self.sum_t0p0,self.sum_t0)\n",
        "        rec0  = ops.divide_no_nan(self.sum_t0p0,self.sum_p0)\n",
        "        prec1 = ops.divide_no_nan(self.sum_t1p1,self.sum_t1)\n",
        "        rec1  = ops.divide_no_nan(self.sum_t1p1,self.sum_p1)\n",
        "        f1_0  = 2.0*ops.divide_no_nan(prec0*rec0,prec0+rec0)\n",
        "        f1_1  = 2.0*ops.divide_no_nan(prec1*rec1,prec1+rec1)\n",
        "        self.f1.assign((f1_0+f1_1)/2.0)\n",
        "        return self.f1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function"
      ],
      "metadata": {
        "id": "XSak6Pz3HIbA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gcCFvAiTY-qK"
      },
      "outputs": [],
      "source": [
        "def c2_f1_loss(y_true, y_pred):\n",
        "    t0 = -1 * (y_true - 1) + 0.0\n",
        "    t1 = y_true\n",
        "    p0 = y_pred[:, 0]\n",
        "    p1 = y_pred[:, 1]\n",
        "    e10 = t1*p0\n",
        "    e01 = t0*p1\n",
        "    e = ops.sum(e10) + ops.sum(e01)\n",
        "    x1 = ops.sum(t1 * p0) / e\n",
        "    x2 = ops.sum(t0 * p1) / e\n",
        "    f1 = 1.0 - 2.0 * (x1 * x2) / (x1 + x2)\n",
        "    return ops.sum(f1 * e10) + ops.sum(f1 * e01)\n",
        "\n",
        "def data2dict(raw):\n",
        "    F = {}\n",
        "    for row in raw:\n",
        "        feature, cell = row[:2]\n",
        "        if feature not in F:       F[feature] = {}\n",
        "        if cell not in F[feature]: F[feature][cell] = []\n",
        "        F[feature][cell] += [row[2:]]\n",
        "    return F\n",
        "\n",
        "def split_num(s):\n",
        "    match = re.match(r\"([a-z]*)([0-9]+)([a-z]*)\", s, re.I)\n",
        "    if match:\n",
        "        items = list(match.groups())\n",
        "    else:\n",
        "        items = [s]\n",
        "    while items[-1] == '': items = items[:-1]\n",
        "    return items\n",
        "\n",
        "def conv(s):\n",
        "    ss = split_num(s)\n",
        "    t = ''\n",
        "    for x in ss:\n",
        "        if x.isdigit():\n",
        "            t += chr(int(x) + ord('z') + 1)\n",
        "        else:\n",
        "            t += x\n",
        "    return t\n",
        "\n",
        "def edit_sim(s1, s2, w=[0, 1, 1, 1]):\n",
        "    s1 = conv(s1.replace('+', '').replace('-', ''))\n",
        "    s2 = conv(s2.replace('+', '').replace('-', ''))\n",
        "    if len(s1) < len(s2): s1, s2 = s2, s1\n",
        "    u, v = len(s1), len(s2)\n",
        "    d = [[0 for col in range(v + 1)] for row in range(u + 1)]\n",
        "    for i in range(0, u + 1): d[i][0] = i\n",
        "    for j in range(0, v + 1): d[0][j] = j\n",
        "    for j in range(1, v + 1):\n",
        "        for i in range(1, u + 1):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1] + w[0]\n",
        "            else:\n",
        "                d[i][j] = min(d[i - 1][j] + w[1], d[i][j - 1] + w[2], d[i - 1][j - 1] + w[3])\n",
        "    return 1.0 - d[u][v] / max(len(s1), len(s2))\n",
        "\n",
        "# given sequences F[feature][cell] = [['RUNX2+',...],[],[],...] with len n, up-sample to target value m>n\n",
        "# up sampling should use a generative model, something like a kmer=3 distribution...\n",
        "def upsample_seqs(teqs, m, strings=True):\n",
        "    if strings:\n",
        "        seqs = [seq.split(' ') for seq in teqs]\n",
        "    else:\n",
        "        seqs = teqs\n",
        "    H = {}\n",
        "    for seq in seqs:\n",
        "        if len(seqs) >= 2:\n",
        "            for i in range(0, len(seq) - 1, 1):\n",
        "                a, b = seq[i], seq[i + 1]\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "            for i in range(len(seq) - 1, 0, -1):\n",
        "                a, b = seq[i].replace('+', '-'), seq[i - 1].replace('-', '+')\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "    G, H = [], {h: {c: H[h][c] for c in sorted(H[h])} for h in sorted(H)}\n",
        "    ss = list(np.random.choice(list(H), m, replace=True))\n",
        "    for i in range(len(ss)):\n",
        "        s, g, n = ss[i], [], np.random.choice([len(seq) for seq in seqs])\n",
        "        for i in range(n):\n",
        "            g += [s]\n",
        "            if s in H:\n",
        "                s = np.random.choice(list(H[s]))\n",
        "            else:\n",
        "                s = np.random.choice(ss)\n",
        "        G += [g]\n",
        "    if strings: G = [' '.join(seq) for seq in G]\n",
        "    return G\n",
        "\n",
        "def get_cell_idx(D):\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    return cell_idx\n",
        "\n",
        "def data_partition(D, split=0.3, shuffle=True,\n",
        "                    balance_w=None):  # will take 70% for training, 20% for test and 10% for validation of each stratification, using spilt=0.3\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = {}, {}, {}, {}, {}, {}\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    print('using data labeling scheme: %s'%cell_idx)\n",
        "    for feature in D:\n",
        "        train_x[feature], train_y[feature] = {}, {}\n",
        "        test_x[feature], test_y[feature] = {}, {}\n",
        "        valid_x[feature], valid_y[feature] = {}, {}\n",
        "        for cell in sorted(D[feature]):\n",
        "            if cell in cell_idx:\n",
        "                n = len(D[feature][cell])\n",
        "                u = int(round(n * (1.0 - split)))\n",
        "                v = int(round(2 * n * split / 3.0))\n",
        "                u_idx = np.random.choice(range(n), u, replace=False)\n",
        "                v_idx = np.random.choice(list(set(range(n)).difference(set(u_idx))), v, replace=False)\n",
        "                w_idx = set(range(n)).difference(set(u_idx).union(set(v_idx)))\n",
        "                train_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in u_idx]\n",
        "                test_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in v_idx]\n",
        "                valid_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in w_idx]\n",
        "\n",
        "    if balance_w is not None:  # balance the data using a variable 0.0-1.0 up/down sampling point\n",
        "        print('balancing data...')\n",
        "        for feature in train_x:\n",
        "            min_class = min([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            max_class = max([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(train_x[feature]):\n",
        "                print('balancing training feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(train_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    train_x[feature][cell] = [train_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    train_x[feature][cell] += upsample_seqs(train_x[feature][cell], mid_class - fc_len,\n",
        "                                                            strings=True)\n",
        "        for feature in test_x:\n",
        "            min_class = min([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            max_class = max([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(test_x[feature]):\n",
        "                print('balancing test feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(test_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    test_x[feature][cell] = [test_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    test_x[feature][cell] += upsample_seqs(test_x[feature][cell], mid_class - fc_len, strings=True)\n",
        "        # for feature in valid_x:\n",
        "        #     min_class = min([len(valid_x[feature][cell]) for cell in valid_x[feature]])\n",
        "        #     max_class = max([len(valid_x[feature][cell]) for cell in valid_x[feature]])\n",
        "        #     mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "        #     for cell in valid_x[feature]:\n",
        "        #         print('balancing valid feature=%s, cell=%s' % (feature, cell))\n",
        "        #         fc_len = len(valid_x[feature][cell])\n",
        "        #         if fc_len >= mid_class:\n",
        "        #             valid_x[feature][cell] = [valid_x[feature][cell][i] for i in\n",
        "        #                                       np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "        #         else:\n",
        "        #             valid_x[feature][cell] += upsample_seqs(valid_x[feature][cell], mid_class - fc_len,\n",
        "        #                                                     strings=True)\n",
        "\n",
        "    for feature in train_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(train_x[feature]):\n",
        "            X += train_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(train_x[feature][cell]))]\n",
        "        train_x[feature], train_y[feature] = X, Y\n",
        "    for feature in test_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(test_x[feature]):\n",
        "            X += test_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(test_x[feature][cell]))]\n",
        "        test_x[feature], test_y[feature] = X, Y\n",
        "        X, Y = [], []\n",
        "    for feature in valid_x:\n",
        "        for cell in sorted(valid_x[feature]):\n",
        "            X += valid_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(valid_x[feature][cell]))]\n",
        "        valid_x[feature], valid_y[feature] = X, Y\n",
        "\n",
        "    if shuffle:  # have the correct number of examples but the labels are not shuffled\n",
        "        for feature in D:\n",
        "            n = len(train_x[feature])\n",
        "            idx = np.random.choice(range(n), n, replace=False)\n",
        "            train_x[feature] = [train_x[feature][i] for i in idx]\n",
        "            train_y[feature] = [train_y[feature][i] for i in idx]\n",
        "\n",
        "            u = len(test_x[feature])\n",
        "            idx = np.random.choice(range(u), u, replace=False)\n",
        "            test_x[feature] = [test_x[feature][i] for i in idx]\n",
        "            test_y[feature] = [test_y[feature][i] for i in idx]\n",
        "\n",
        "            v = len(valid_x[feature])\n",
        "            idx = np.random.choice(range(v), v, replace=False)\n",
        "            valid_x[feature] = [valid_x[feature][i] for i in idx]\n",
        "            valid_y[feature] = [valid_y[feature][i] for i in idx]\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
        "\n",
        "def preprocess_data(D, max_vocab=None, maxlen=2000, split=0.4, balance_w=0.5, dt=np.float32):\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = data_partition(D, split=split, balance_w=balance_w)\n",
        "    vocab = sorted(get_vocab(D))\n",
        "    vocab_size = len(vocab)\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=\"lower\",\n",
        "        max_tokens=min(vocab_size, max_vocab),\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=maxlen\n",
        "    )\n",
        "    vectorize_layer.adapt(vocab)\n",
        "    train, test, valid = {}, {}, {}\n",
        "    for feature in D:\n",
        "        train[feature] = (\n",
        "        np.array(vectorize_layer(train_x[feature]), dtype=dt), np.array(train_y[feature], dtype=dt))\n",
        "        test[feature] = (np.array(vectorize_layer(test_x[feature]), dtype=dt), np.array(test_y[feature], dtype=dt))\n",
        "        valid[feature] = (\n",
        "        np.array(vectorize_layer(valid_x[feature]), dtype=dt), np.array(valid_y[feature], dtype=dt))\n",
        "    return train, test, valid, vectorize_layer\n",
        "\n",
        "def get_vocab(D):\n",
        "    V = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            for row in D[feature][cell]:\n",
        "                for i in row:\n",
        "                    V.add(i)\n",
        "    return V\n",
        "\n",
        "def get_trans(C, ks=16):\n",
        "    S = {}\n",
        "    V = sorted(set([re.split('[+|-]', v)[0] for v in get_vocab(C)]))\n",
        "    idx = {V[i]: i for i in range(len(V))}\n",
        "    for feature in C:\n",
        "        S[feature] = {}\n",
        "        for cell in C[feature]:\n",
        "            S[feature][cell] = []\n",
        "            for i in range(len(C[feature][cell])):\n",
        "                X = np.zeros((len(idx), len(idx), 3), dtype=float)\n",
        "                for k in range(min(ks, len(C[feature][cell][i]))):\n",
        "                    for a in range(len(C[feature][cell][i]) - k):\n",
        "                        tf1 = re.split('[+|-]', C[feature][cell][i][a])[0]\n",
        "                        tf2 = re.split('[+|-]', C[feature][cell][i][a + k])[0]\n",
        "                        if C[feature][cell][i][a].find('+') > -1:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                        else:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                X /= np.sum(X)\n",
        "                S[feature][cell] += [X]\n",
        "    return S\n",
        "\n",
        "def get_classes(D):\n",
        "    C = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            C.add(cell)\n",
        "    return C\n",
        "\n",
        "def cell_map(F, cmap):\n",
        "    C = {}\n",
        "    for feature in F:\n",
        "        C[feature] = {}\n",
        "        for cell in F[feature]:\n",
        "            if cmap[cell] in C[feature]:\n",
        "                C[feature][cmap[cell]] += [row for row in F[feature][cell]]\n",
        "            else:\n",
        "                C[feature][cmap[cell]] = [row for row in F[feature][cell]]\n",
        "    return C\n",
        "\n",
        "def confusion_matrix(T, Y, c=10):\n",
        "    M = np.array([[0.0 for j in range(c)] for i in range(c)], dtype=float)\n",
        "    for i in range(len(T)):\n",
        "        M[int(T[i])][int(Y[i])] += 1.0\n",
        "    return M\n",
        "\n",
        "def f1_score(cm):\n",
        "    f1s = []\n",
        "    for i in range(len(cm)):\n",
        "        prec = (0.0 if np.sum(cm[:, i]) == 0.0 else cm[i, i] / np.sum(cm[:, i]))\n",
        "        rec = (0.0 if np.sum(cm[i, :]) == 0.0 else cm[i, i] / np.sum(cm[i, :]))\n",
        "        f1s += [(0.0 if (prec + rec) == 0.0 else 2 * (prec * rec) / (prec + rec))]\n",
        "    return (0.0 if () == 0.0 else len(cm) * (np.prod(f1s)) / (np.sum(f1s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WvPDUKD8HM2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the raw data sequences as: feature,cell,x1,x2,...xm\n",
        "with gzip.GzipFile(in_path, 'rb') as f:\n",
        "    raw = [row.decode('utf-8').replace('\\n', '').split('\\t') for row in f.readlines()]\n",
        "F = data2dict(raw)"
      ],
      "metadata": {
        "id": "Nh-Zyc_JKDEs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(F['AEC']))"
      ],
      "metadata": {
        "id": "etOrTXsZLxS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad9wHZ06ZF9e"
      },
      "outputs": [],
      "source": [
        "# read in the raw data sequences as: feature,cell,x1,x2,...xm\n",
        "with gzip.GzipFile(in_path, 'rb') as f:\n",
        "    raw = [row.decode('utf-8').replace('\\n', '').split('\\t') for row in f.readlines()]\n",
        "F = data2dict(raw)\n",
        "\n",
        "# apply cell map to F\n",
        "cmap = {'MESENCHYMAL': 'OFC', 'NEURALCREST': 'OFC', 'OSTEOCYTE': 'BACKGROUND',\n",
        "        'HEPATOCYTE': 'BACKGROUND', 'MESOTHELIALEPICARDIUM': 'BACKGROUND', 'NEPHRONPROGENITOR': 'BACKGROUND',\n",
        "        'PANCREAS': 'BACKGROUND', 'PERIPHERALBLOOD': 'BACKGROUND', 'SPLEEN': 'BACKGROUND', 'STOMACH': 'BACKGROUND'}\n",
        "# cmap = {k:k for k in set([c for f in F for c in F[f]])}\n",
        "C = cell_map(F, cmap)\n",
        "C = {feature: C[feature] for feature in features}  # apply only features that are selected\n",
        "\n",
        "#embed_dims,num_heads,ff_dims,batches,drops = [4],[2],[16],[128],[0.3]\n",
        "embed_dims,num_heads,ff_dims,batches,drops = [2,4,8],[2,4,8],[16,32,64],[32,64,128],[0.4,0.5]\n",
        "hyper_params = []\n",
        "for embed_dim in embed_dims:\n",
        "    for num_head in num_heads:\n",
        "        for ff_dim in ff_dims:\n",
        "            for batch in batches:\n",
        "                for drop in drops:\n",
        "                    hyper_params += [[embed_dim, num_head, ff_dim, batch, drop, balance_w]]\n",
        "\n",
        "class_num = len(get_classes(C))\n",
        "vocab_size = len(sorted(get_vocab(C)))\n",
        "max_vocab = min(max_vocab, vocab_size)\n",
        "\n",
        "print('preprocessing data now...')\n",
        "train, test, valid, vec = preprocess_data(C, max_vocab=max_vocab, maxlen=maxlen, split=split, balance_w=balance_w)\n",
        "\n",
        "cell_idx = get_cell_idx(C)\n",
        "cell_label_str = '_'.join([k+'-%s'%cell_idx[k] for k in sorted(cell_idx)])\n",
        "json_score = '/'.join(in_path.split('/')[:-1])+'/%s.%s.%s.score.json'%('_'.join(sorted(features)),cell_label_str,balance_w)\n",
        "if os.path.exists(json_score):\n",
        "    with open(json_score,'r') as f: score = json.load(f)\n",
        "else: score = {feature: [hyper_params[0],[], 0.0] for feature in C}\n",
        "\n",
        "models = {}\n",
        "for feature in C:\n",
        "    for hyper in hyper_params:\n",
        "        embed_dim, num_head, ff_dim, batch, drop, bl = hyper\n",
        "        with tf.device('/device:GPU:%s'%gpu_num):\n",
        "            # -----------------------------------------------------------------------\n",
        "            inputs = layers.Input(shape=(maxlen,))\n",
        "            embedding_layer = TokenAndPositionEmbedding(maxlen, max_vocab, embed_dim)\n",
        "            x = embedding_layer(inputs)\n",
        "            transformer_block = TransformerBlock(embed_dim, num_head, ff_dim)\n",
        "            x = transformer_block(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "            x = layers.Dropout(0.2)(x)\n",
        "            x = layers.Dense(class_num * 64, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(drop)(x)\n",
        "            outputs = layers.Dense(class_num, activation=\"softmax\")(x)\n",
        "            models[feature] = keras.Model(inputs=inputs, outputs=outputs)\n",
        "            models[feature].compile(optimizer=keras.optimizers.Adam(5e-5),\n",
        "                                    loss=c2_f1_loss,\n",
        "                                    metrics=[F1()])\n",
        "            models[feature].name = feature\n",
        "\n",
        "            # training-----------------------------------------------------------------------\n",
        "            callbacks = [\n",
        "                keras.callbacks.EarlyStopping(monitor=\"val_f1\",\n",
        "                                                min_delta=1e-2,patience=5,mode='max',verbose=1,\n",
        "                                                restore_best_weights=True)\n",
        "            ]\n",
        "            history = models[feature].fit(\n",
        "                x=train[feature][0], y=train[feature][1], epochs=100,\n",
        "                validation_data=(test[feature][0],test[feature][1])\n",
        "            )\n",
        "\n",
        "            # validation score-----------------------------------------------------------------------\n",
        "            true = valid[feature][1]\n",
        "            pred = np.argmax(models[feature].predict(valid[feature][0]), axis=1)\n",
        "            cm = confusion_matrix(true, pred, c=class_num)\n",
        "            cm0 = (cm if np.sum(cm) == 0.0 else cm / np.sum(cm))\n",
        "            print(cm)\n",
        "            print(cm0)\n",
        "            f1 = f1_score(cm)\n",
        "            if f1 >= score[feature][-1]:\n",
        "                print(\n",
        "                    '-------------------------------new high f1 score: feature=%s, balance=%s, f1=%s -------------------------------\\n%s'\\\n",
        "                    %(feature,balance_w,f1,hyper))\n",
        "                score[feature] = [hyper, [list(l) for l in list(cm)], f1]\n",
        "                weight_path    = '/'.join(in_path.split('/')[:-1])+'/%s.%s.%s.weights.h5'%(feature,cell_label_str,balance_w)\n",
        "                if os.path.exists(json_score):\n",
        "                    with open(json_score,'r') as f:  old_score          = json.load(f)\n",
        "                    if feature in old_score:         old_score[feature] = score[feature]\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "                else:\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "            else:\n",
        "                print('f1 score: feature=%s, balance=%s, f1=%s'%(feature,balance_w, f1))\n",
        "\n",
        "            print({cell: len(F[feature][cell]) for cell in F[feature]})\n",
        "            min_score = max([len(F[feature][cell]) for cell in F[feature]]) / sum(\n",
        "                [len(F[feature][cell]) for cell in F[feature]])\n",
        "            print(min_score)\n",
        "            print({cell: len(C[feature][cell]) for cell in C[feature]})\n",
        "            min_score = max([len(C[feature][cell]) for cell in C[feature]]) / sum(\n",
        "                [len(C[feature][cell]) for cell in C[feature]])\n",
        "            print(min_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}