{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-psvXpKG_x8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D9OdA0JYgif",
        "outputId": "7c38eeba-b738-47fb-c79c-88090fe37aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e: print(e)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NFwoVcfHYx7",
        "outputId": "3bf31f8a-f524-4c30-cc25-fd274a931c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_DIR=\"/content/drive/MyDrive/Shared/TFBS_Cell_project/\"\n",
        "in_path=ROOT_DIR+\"cellvar.db.tfbs_seq.tsv.gz\"\n",
        "out_dir=ROOT_DIR+\"out\"\n",
        "\n",
        "if path.exists(out_dir) == False:\n",
        "    os.mkdir(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "58aFffPZIVLA"
      },
      "outputs": [],
      "source": [
        "# comma-seperated list fo features to model: AEC,PEC,PC,IC\n",
        "features=[\"AEC\"]\n",
        "\n",
        "# split factor [0.0 to 1.0]\n",
        "split=0.3\n",
        "\n",
        "# maximum vocabulary size\n",
        "max_vocab=5\n",
        "\n",
        "# sequence length\n",
        "maxlen=50\n",
        "\n",
        "# balance factor [0.0 is downsample to 1.0 is upsample]\n",
        "balance_w=None\n",
        "\n",
        "# gpu number [0 to x]\n",
        "gpu_num=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "EiRPOh24YoMS"
      },
      "outputs": [],
      "source": [
        "# des = \"\"\"\n",
        "# TFBS sequence training/learning tool\n",
        "# (C) Timothy James Becker, 05/03/24-07/08/24, version=0.0.1\"\"\"\n",
        "# parser = argparse.ArgumentParser(description=des, formatter_class=argparse.RawTextHelpFormatter)\n",
        "# # need the split, max vocab, maxlen, balance, cmap,tsv.gz input file\n",
        "# parser.add_argument('--in_path', type=str, help='cellvar.db.tfbs_seq.tsv.gz traing input file')\n",
        "# parser.add_argument('--out_dir', type=str, help='output directory')\n",
        "# parser.add_argument('--features', type=str, help='comma-seperated list fo features to model: AEC,PEC,PC,IC')\n",
        "# parser.add_argument('--split', type=float, help='split factor [0.0 to 1.0]')\n",
        "# parser.add_argument('--vocab', type=int, help='maximum vocabulary size')\n",
        "# parser.add_argument('--len', type=int, help='sequence length')\n",
        "# parser.add_argument('--balance', type=float, help='balance factor [0.0 is downsample to 1.0 is upsample]')\n",
        "# parser.add_argument('--gpu', type=int, help='gpu number [0 to x]')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# in_path = args.in_path\n",
        "# features = args.features.split(',')\n",
        "# features = set([feature.upper() for feature in features]).intersection(set(['AEC', 'PEC', 'PC', 'IC']))\n",
        "# split = args.split\n",
        "# max_vocab = args.vocab\n",
        "# maxlen = args.len\n",
        "# balance_w = args.balance\n",
        "# gpu_num = args.gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YRp0xKLHDAI"
      },
      "source": [
        "## Network layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NHNmYBMAYy48"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim,self.num_heads,self.ff_dim,self.rate = embed_dim, num_heads, ff_dim, rate\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.maxlen,self.vocab_size,self.embed_dim = maxlen, vocab_size, embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z_yeyVpJY5Qa"
      },
      "outputs": [],
      "source": [
        "class F1(keras.metrics.Metric):\n",
        "    def __init__(self, name='f1'):\n",
        "        super().__init__(name=name)\n",
        "        self.sum_t0   = self.add_variable(shape=(),initializer='zeros',name='sum_t0')\n",
        "        self.sum_p0   = self.add_variable(shape=(),initializer='zeros',name='sum_p0')\n",
        "        self.sum_t0p0 = self.add_variable(shape=(),initializer='zeros',name='sum_t0p0')\n",
        "        self.sum_t1   = self.add_variable(shape=(),initializer='zeros',name='sum_t1')\n",
        "        self.sum_p1   = self.add_variable(shape=(),initializer='zeros',name='sum_p1')\n",
        "        self.sum_t1p1 = self.add_variable(shape=(),initializer='zeros',name='sum_t1p1')\n",
        "        self.f1       = self.add_variable(shape=(),initializer='zeros',name='f1')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        t0 = -1 * (y_true - 1) + 0.0\n",
        "        t1 = y_true\n",
        "        p0 = ops.round(y_pred[:, 0],0)\n",
        "        p1 = ops.round(y_pred[:, 1],0)\n",
        "        self.sum_t0.assign(self.sum_t0+ops.sum(t0))\n",
        "        self.sum_p0.assign(self.sum_p0+ops.sum(p0))\n",
        "        self.sum_t0p0.assign(self.sum_t0p0+ops.sum(t0*p0))\n",
        "        self.sum_t1.assign( self.sum_t1+ops.sum(t1))\n",
        "        self.sum_p1.assign(self.sum_p1+ops.sum(p1))\n",
        "        self.sum_t1p1.assign(self.sum_t1p1+ops.sum(t1*p1))\n",
        "\n",
        "    def result(self):\n",
        "        prec0 = ops.divide_no_nan(self.sum_t0p0,self.sum_t0)\n",
        "        rec0  = ops.divide_no_nan(self.sum_t0p0,self.sum_p0)\n",
        "        prec1 = ops.divide_no_nan(self.sum_t1p1,self.sum_t1)\n",
        "        rec1  = ops.divide_no_nan(self.sum_t1p1,self.sum_p1)\n",
        "        f1_0  = 2.0*ops.divide_no_nan(prec0*rec0,prec0+rec0)\n",
        "        f1_1  = 2.0*ops.divide_no_nan(prec1*rec1,prec1+rec1)\n",
        "        self.f1.assign((f1_0+f1_1)/2.0)\n",
        "        return self.f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSak6Pz3HIbA"
      },
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gcCFvAiTY-qK"
      },
      "outputs": [],
      "source": [
        "def c2_f1_loss(y_true, y_pred):\n",
        "    t0 = -1 * (y_true - 1) + 0.0\n",
        "    t1 = y_true\n",
        "    p0 = y_pred[:, 0]\n",
        "    p1 = y_pred[:, 1]\n",
        "    e10 = t1*p0\n",
        "    e01 = t0*p1\n",
        "    e = ops.sum(e10) + ops.sum(e01)\n",
        "    x1 = ops.sum(t1 * p0) / e\n",
        "    x2 = ops.sum(t0 * p1) / e\n",
        "    f1 = 1.0 - 2.0 * (x1 * x2) / (x1 + x2)\n",
        "    return ops.sum(f1 * e10) + ops.sum(f1 * e01)\n",
        "\n",
        "def data2dict(raw):\n",
        "    F = {}\n",
        "    for row in raw:\n",
        "        feature, cell = row[:2]\n",
        "        if feature not in F:       F[feature] = {}\n",
        "        if cell not in F[feature]: F[feature][cell] = []\n",
        "        F[feature][cell] += [row[2:]]\n",
        "    return F\n",
        "\n",
        "def split_num(s):\n",
        "    match = re.match(r\"([a-z]*)([0-9]+)([a-z]*)\", s, re.I)\n",
        "    if match:\n",
        "        items = list(match.groups())\n",
        "    else:\n",
        "        items = [s]\n",
        "    while items[-1] == '': items = items[:-1]\n",
        "    return items\n",
        "\n",
        "def conv(s):\n",
        "    ss = split_num(s)\n",
        "    t = ''\n",
        "    for x in ss:\n",
        "        if x.isdigit():\n",
        "            t += chr(int(x) + ord('z') + 1)\n",
        "        else:\n",
        "            t += x\n",
        "    return t\n",
        "\n",
        "def edit_sim(s1, s2, w=[0, 1, 1, 1]):\n",
        "    s1 = conv(s1.replace('+', '').replace('-', ''))\n",
        "    s2 = conv(s2.replace('+', '').replace('-', ''))\n",
        "    if len(s1) < len(s2): s1, s2 = s2, s1\n",
        "    u, v = len(s1), len(s2)\n",
        "    d = [[0 for col in range(v + 1)] for row in range(u + 1)]\n",
        "    for i in range(0, u + 1): d[i][0] = i\n",
        "    for j in range(0, v + 1): d[0][j] = j\n",
        "    for j in range(1, v + 1):\n",
        "        for i in range(1, u + 1):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1] + w[0]\n",
        "            else:\n",
        "                d[i][j] = min(d[i - 1][j] + w[1], d[i][j - 1] + w[2], d[i - 1][j - 1] + w[3])\n",
        "    return 1.0 - d[u][v] / max(len(s1), len(s2))\n",
        "\n",
        "# given sequences F[feature][cell] = [['RUNX2+',...],[],[],...] with len n, up-sample to target value m>n\n",
        "# up sampling should use a generative model, something like a kmer=3 distribution...\n",
        "def upsample_seqs(teqs, m, strings=True):\n",
        "    if strings:\n",
        "        seqs = [seq.split(' ') for seq in teqs]\n",
        "    else:\n",
        "        seqs = teqs\n",
        "    H = {}\n",
        "    for seq in seqs:\n",
        "        if len(seqs) >= 2:\n",
        "            for i in range(0, len(seq) - 1, 1):\n",
        "                a, b = seq[i], seq[i + 1]\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "            for i in range(len(seq) - 1, 0, -1):\n",
        "                a, b = seq[i].replace('+', '-'), seq[i - 1].replace('-', '+')\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "    G, H = [], {h: {c: H[h][c] for c in sorted(H[h])} for h in sorted(H)}\n",
        "    ss = list(np.random.choice(list(H), m, replace=True))\n",
        "    for i in range(len(ss)):\n",
        "        s, g, n = ss[i], [], np.random.choice([len(seq) for seq in seqs])\n",
        "        for i in range(n):\n",
        "            g += [s]\n",
        "            if s in H:\n",
        "                s = np.random.choice(list(H[s]))\n",
        "            else:\n",
        "                s = np.random.choice(ss)\n",
        "        G += [g]\n",
        "    if strings: G = [' '.join(seq) for seq in G]\n",
        "    return G\n",
        "\n",
        "def get_cell_idx(D):\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    return cell_idx\n",
        "\n",
        "def data_partition(D, split=0.3, shuffle=True,\n",
        "                    balance_w=None):  # will take 70% for training, 20% for test and 10% for validation of each stratification, using spilt=0.3\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = {}, {}, {}, {}, {}, {}\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    print('using data labeling scheme: %s'%cell_idx)\n",
        "    for feature in D:\n",
        "        train_x[feature], train_y[feature] = {}, {}\n",
        "        test_x[feature], test_y[feature] = {}, {}\n",
        "        valid_x[feature], valid_y[feature] = {}, {}\n",
        "        for cell in sorted(D[feature]):\n",
        "            if cell in cell_idx:\n",
        "                n = len(D[feature][cell])\n",
        "                u = int(round(n * (1.0 - split)))\n",
        "                v = int(round(2 * n * split / 3.0))\n",
        "                u_idx = np.random.choice(range(n), u, replace=False)\n",
        "                v_idx = np.random.choice(list(set(range(n)).difference(set(u_idx))), v, replace=False)\n",
        "                w_idx = set(range(n)).difference(set(u_idx).union(set(v_idx)))\n",
        "                train_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in u_idx]\n",
        "                test_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in v_idx]\n",
        "                valid_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in w_idx]\n",
        "\n",
        "    if balance_w is not None:  # balance the data using a variable 0.0-1.0 up/down sampling point\n",
        "        print('balancing data...')\n",
        "        for feature in train_x:\n",
        "            min_class = min([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            max_class = max([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(train_x[feature]):\n",
        "                print('balancing training feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(train_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    train_x[feature][cell] = [train_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    train_x[feature][cell] += upsample_seqs(train_x[feature][cell], mid_class - fc_len,\n",
        "                                                            strings=True)\n",
        "        for feature in test_x:\n",
        "            min_class = min([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            max_class = max([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(test_x[feature]):\n",
        "                print('balancing test feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(test_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    test_x[feature][cell] = [test_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    test_x[feature][cell] += upsample_seqs(test_x[feature][cell], mid_class - fc_len, strings=True)\n",
        "        # for feature in valid_x:\n",
        "        #     min_class = min([len(valid_x[feature][cell]) for cell in valid_x[feature]])\n",
        "        #     max_class = max([len(valid_x[feature][cell]) for cell in valid_x[feature]])\n",
        "        #     mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "        #     for cell in valid_x[feature]:\n",
        "        #         print('balancing valid feature=%s, cell=%s' % (feature, cell))\n",
        "        #         fc_len = len(valid_x[feature][cell])\n",
        "        #         if fc_len >= mid_class:\n",
        "        #             valid_x[feature][cell] = [valid_x[feature][cell][i] for i in\n",
        "        #                                       np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "        #         else:\n",
        "        #             valid_x[feature][cell] += upsample_seqs(valid_x[feature][cell], mid_class - fc_len,\n",
        "        #                                                     strings=True)\n",
        "\n",
        "    for feature in train_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(train_x[feature]):\n",
        "            X += train_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(train_x[feature][cell]))]\n",
        "        train_x[feature], train_y[feature] = X, Y\n",
        "    for feature in test_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(test_x[feature]):\n",
        "            X += test_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(test_x[feature][cell]))]\n",
        "        test_x[feature], test_y[feature] = X, Y\n",
        "        X, Y = [], []\n",
        "    for feature in valid_x:\n",
        "        for cell in sorted(valid_x[feature]):\n",
        "            X += valid_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(valid_x[feature][cell]))]\n",
        "        valid_x[feature], valid_y[feature] = X, Y\n",
        "\n",
        "    if shuffle:  # have the correct number of examples but the labels are not shuffled\n",
        "        for feature in D:\n",
        "            n = len(train_x[feature])\n",
        "            idx = np.random.choice(range(n), n, replace=False)\n",
        "            train_x[feature] = [train_x[feature][i] for i in idx]\n",
        "            train_y[feature] = [train_y[feature][i] for i in idx]\n",
        "\n",
        "            u = len(test_x[feature])\n",
        "            idx = np.random.choice(range(u), u, replace=False)\n",
        "            test_x[feature] = [test_x[feature][i] for i in idx]\n",
        "            test_y[feature] = [test_y[feature][i] for i in idx]\n",
        "\n",
        "            v = len(valid_x[feature])\n",
        "            idx = np.random.choice(range(v), v, replace=False)\n",
        "            valid_x[feature] = [valid_x[feature][i] for i in idx]\n",
        "            valid_y[feature] = [valid_y[feature][i] for i in idx]\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
        "\n",
        "def preprocess_data(D, max_vocab=None, maxlen=2000, split=0.4, balance_w=0.5, dt=np.float32):\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = data_partition(D, split=split, balance_w=balance_w)\n",
        "    vocab = sorted(get_vocab(D))\n",
        "    vocab_size = len(vocab)\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=\"lower\",\n",
        "        max_tokens=min(vocab_size, max_vocab),\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=maxlen\n",
        "    )\n",
        "    vectorize_layer.adapt(vocab)\n",
        "    train, test, valid = {}, {}, {}\n",
        "    for feature in D:\n",
        "        train[feature] = (\n",
        "        np.array(vectorize_layer(train_x[feature]), dtype=dt), np.array(train_y[feature], dtype=dt))\n",
        "        test[feature] = (np.array(vectorize_layer(test_x[feature]), dtype=dt), np.array(test_y[feature], dtype=dt))\n",
        "        valid[feature] = (\n",
        "        np.array(vectorize_layer(valid_x[feature]), dtype=dt), np.array(valid_y[feature], dtype=dt))\n",
        "    return train, test, valid, vectorize_layer\n",
        "\n",
        "def get_vocab(D):\n",
        "    V = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            for row in D[feature][cell]:\n",
        "                for i in row:\n",
        "                    V.add(i)\n",
        "    return V\n",
        "\n",
        "def get_trans(C, ks=16):\n",
        "    S = {}\n",
        "    V = sorted(set([re.split('[+|-]', v)[0] for v in get_vocab(C)]))\n",
        "    idx = {V[i]: i for i in range(len(V))}\n",
        "    for feature in C:\n",
        "        S[feature] = {}\n",
        "        for cell in C[feature]:\n",
        "            S[feature][cell] = []\n",
        "            for i in range(len(C[feature][cell])):\n",
        "                X = np.zeros((len(idx), len(idx), 3), dtype=float)\n",
        "                for k in range(min(ks, len(C[feature][cell][i]))):\n",
        "                    for a in range(len(C[feature][cell][i]) - k):\n",
        "                        tf1 = re.split('[+|-]', C[feature][cell][i][a])[0]\n",
        "                        tf2 = re.split('[+|-]', C[feature][cell][i][a + k])[0]\n",
        "                        if C[feature][cell][i][a].find('+') > -1:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                        else:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                X /= np.sum(X)\n",
        "                S[feature][cell] += [X]\n",
        "    return S\n",
        "\n",
        "def get_classes(D):\n",
        "    C = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            C.add(cell)\n",
        "    return C\n",
        "\n",
        "def cell_map(F, cmap):\n",
        "    C = {}\n",
        "    for feature in F:\n",
        "        C[feature] = {}\n",
        "        for cell in F[feature]:\n",
        "            if cmap[cell] in C[feature]:\n",
        "                C[feature][cmap[cell]] += [row for row in F[feature][cell]]\n",
        "            else:\n",
        "                C[feature][cmap[cell]] = [row for row in F[feature][cell]]\n",
        "    return C\n",
        "\n",
        "def confusion_matrix(T, Y, c=10):\n",
        "    M = np.array([[0.0 for j in range(c)] for i in range(c)], dtype=float)\n",
        "    for i in range(len(T)):\n",
        "        M[int(T[i])][int(Y[i])] += 1.0\n",
        "    return M\n",
        "\n",
        "def f1_score(cm):\n",
        "    f1s = []\n",
        "    for i in range(len(cm)):\n",
        "        prec = (0.0 if np.sum(cm[:, i]) == 0.0 else cm[i, i] / np.sum(cm[:, i]))\n",
        "        rec = (0.0 if np.sum(cm[i, :]) == 0.0 else cm[i, i] / np.sum(cm[i, :]))\n",
        "        f1s += [(0.0 if (prec + rec) == 0.0 else 2 * (prec * rec) / (prec + rec))]\n",
        "    return (0.0 if () == 0.0 else len(cm) * (np.prod(f1s)) / (np.sum(f1s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvPDUKD8HM2v"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the raw data sequences as: feature,cell,x1,x2,...xm\n",
        "with gzip.GzipFile(in_path, 'rb') as f:\n",
        "    raw = [row.decode('utf-8').replace('\\n', '').split('\\t') for row in f.readlines()]\n",
        "F = data2dict(raw)"
      ],
      "metadata": {
        "id": "v6_1fGkSRu25"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFWIbNQUK_SY",
        "outputId": "e1250c75-f1b8-44f9-a5d8-807916763e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AEC', 'IC', 'PEC', 'PC']\n",
            "['MESOTHELIALEPICARDIUM', 'SPLEEN', 'STOMACH', 'HEPATOCYTE', 'NEURALCREST', 'NEPHRONPROGENITOR', 'OSTEOCYTE', 'PANCREAS', 'MESENCHYMAL', 'PERIPHERALBLOOD']\n"
          ]
        }
      ],
      "source": [
        "print(list(F))\n",
        "print(list(F['AEC']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nh-Zyc_JKDEs"
      },
      "outputs": [],
      "source": [
        "# apply cell map to F\n",
        "cmap = {'MESENCHYMAL': 'OFC', 'NEURALCREST': 'OFC', 'OSTEOCYTE': 'BACKGROUND',\n",
        "        'HEPATOCYTE': 'BACKGROUND', 'MESOTHELIALEPICARDIUM': 'BACKGROUND', 'NEPHRONPROGENITOR': 'BACKGROUND',\n",
        "        'PANCREAS': 'BACKGROUND', 'PERIPHERALBLOOD': 'BACKGROUND', 'SPLEEN': 'BACKGROUND', 'STOMACH': 'BACKGROUND'}\n",
        "# cmap = {k:k for k in set([c for f in F for c in F[f]])}\n",
        "C = cell_map(F, cmap)\n",
        "C = {feature: C[feature] for feature in features}  # apply only features that are selected"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(C['AEC']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK4Q0ComUOTk",
        "outputId": "2cf42966-7861-4f82-d12a-7456f74b7086"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BACKGROUND', 'OFC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embed_dims,num_heads,ff_dims,batches,drops = [4],[2],[16],[128],[0.3]\n",
        "embed_dims,num_heads,ff_dims,batches,drops = [2,4,8],[2,4,8],[16,32,64],[32,64,128],[0.4,0.5]\n",
        "hyper_params = []\n",
        "for embed_dim in embed_dims:\n",
        "    for num_head in num_heads:\n",
        "        for ff_dim in ff_dims:\n",
        "            for batch in batches:\n",
        "                for drop in drops:\n",
        "                    hyper_params += [[embed_dim, num_head, ff_dim, batch, drop, balance_w]]\n",
        "\n",
        "class_num = len(get_classes(C))\n",
        "vocab_size = len(sorted(get_vocab(C)))\n",
        "max_vocab = min(max_vocab, vocab_size)"
      ],
      "metadata": {
        "id": "OV-NnwJJUYFF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('preprocessing data now...')\n",
        "train, test, valid, vec = preprocess_data(C, max_vocab=max_vocab, maxlen=maxlen, split=split, balance_w=balance_w)"
      ],
      "metadata": {
        "id": "SPDHVFvwV7Qo",
        "outputId": "8b213355-8ec9-41eb-b914-d9993715f3db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing data now...\n",
            "using data labeling scheme: {'BACKGROUND': 0, 'OFC': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ad9wHZ06ZF9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bc3be41-bd6f-42b1-8bd9-774802691727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step - f1: 0.4503 - loss: 5.7894 - val_f1: 0.4430 - val_loss: 5.0052\n",
            "Epoch 2/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4430 - loss: 5.0176 - val_f1: 0.4430 - val_loss: 5.0040\n",
            "Epoch 3/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0386 - val_f1: 0.4430 - val_loss: 5.0059\n",
            "Epoch 4/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4430 - loss: 5.0209 - val_f1: 0.4430 - val_loss: 5.0030\n",
            "Epoch 5/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0495 - val_f1: 0.4430 - val_loss: 5.0030\n",
            "Epoch 6/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4428 - loss: 5.0333 - val_f1: 0.4430 - val_loss: 5.0150\n",
            "Epoch 7/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4431 - loss: 5.0133 - val_f1: 0.4430 - val_loss: 5.0008\n",
            "Epoch 8/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4430 - loss: 5.0080 - val_f1: 0.4430 - val_loss: 5.0164\n",
            "Epoch 9/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0269 - val_f1: 0.4430 - val_loss: 5.0158\n",
            "Epoch 10/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0322 - val_f1: 0.4430 - val_loss: 5.0200\n",
            "Epoch 11/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4429 - loss: 5.0196 - val_f1: 0.4430 - val_loss: 5.0307\n",
            "Epoch 12/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4428 - loss: 5.0279 - val_f1: 0.4430 - val_loss: 5.0167\n",
            "Epoch 13/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0227 - val_f1: 0.4430 - val_loss: 5.0110\n",
            "Epoch 14/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4432 - loss: 4.9782 - val_f1: 0.4430 - val_loss: 5.0075\n",
            "Epoch 15/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4428 - loss: 5.0149 - val_f1: 0.4430 - val_loss: 4.9881\n",
            "Epoch 16/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4428 - loss: 4.9856 - val_f1: 0.4430 - val_loss: 4.9594\n",
            "Epoch 17/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4429 - loss: 4.9500 - val_f1: 0.4430 - val_loss: 4.9379\n",
            "Epoch 18/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4429 - loss: 4.9492 - val_f1: 0.4430 - val_loss: 4.9801\n",
            "Epoch 19/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4426 - loss: 5.0162 - val_f1: 0.4430 - val_loss: 5.0420\n",
            "Epoch 20/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4431 - loss: 4.9999 - val_f1: 0.4430 - val_loss: 5.0561\n",
            "Epoch 21/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0317 - val_f1: 0.4430 - val_loss: 5.0133\n",
            "Epoch 22/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - f1: 0.4429 - loss: 5.0154 - val_f1: 0.4430 - val_loss: 5.0696\n",
            "Epoch 23/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4427 - loss: 5.0147 - val_f1: 0.4430 - val_loss: 5.0438\n",
            "Epoch 24/100\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - f1: 0.4428 - loss: 5.0239 - val_f1: 0.4430 - val_loss: 5.0434\n",
            "Epoch 25/100\n",
            "\u001b[1m7692/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - f1: 0.4434 - loss: 4.9805"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6ffd5177428c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                 restore_best_weights=True)\n\u001b[1;32m     35\u001b[0m             ]\n\u001b[0;32m---> 36\u001b[0;31m             history = models[feature].fit(\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36munpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       flat.extend(\n\u001b[0;32m--> 391\u001b[0;31m           \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_constraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mto_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     nest.map_structure(\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_specs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m   \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    629\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m   \"\"\"\n\u001b[1;32m   1064\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m   return _tf_core_pack_sequence_as(\n\u001b[0m\u001b[1;32m   1104\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_nested_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m     raise TypeError(\n\u001b[1;32m    881\u001b[0m         \u001b[0;34m\"Attempted to pack value:\\n  {}\\ninto a structure, but found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cell_idx = get_cell_idx(C)\n",
        "cell_label_str = '_'.join([k+'-%s'%cell_idx[k] for k in sorted(cell_idx)])\n",
        "json_score = '/'.join(in_path.split('/')[:-1])+'/%s.%s.%s.score.json'%('_'.join(sorted(features)),cell_label_str,balance_w)\n",
        "if os.path.exists(json_score):\n",
        "    with open(json_score,'r') as f: score = json.load(f)\n",
        "else: score = {feature: [hyper_params[0],[], 0.0] for feature in C}\n",
        "\n",
        "models = {}\n",
        "for feature in C:\n",
        "    for hyper in hyper_params:\n",
        "        embed_dim, num_head, ff_dim, batch, drop, bl = hyper\n",
        "        with tf.device('/device:GPU:%s'%gpu_num):\n",
        "            # -----------------------------------------------------------------------\n",
        "            inputs = layers.Input(shape=(maxlen,))\n",
        "            embedding_layer = TokenAndPositionEmbedding(maxlen, max_vocab, embed_dim)\n",
        "            x = embedding_layer(inputs)\n",
        "            transformer_block = TransformerBlock(embed_dim, num_head, ff_dim)\n",
        "            x = transformer_block(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "            x = layers.Dropout(0.2)(x)\n",
        "            x = layers.Dense(class_num * 64, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(drop)(x)\n",
        "            outputs = layers.Dense(class_num, activation=\"softmax\")(x)\n",
        "            models[feature] = keras.Model(inputs=inputs, outputs=outputs)\n",
        "            models[feature].compile(optimizer=keras.optimizers.Adam(5e-5),\n",
        "                                    loss=c2_f1_loss,\n",
        "                                    metrics=[F1()])\n",
        "            models[feature].name = feature\n",
        "\n",
        "            # training-----------------------------------------------------------------------\n",
        "            callbacks = [\n",
        "                keras.callbacks.EarlyStopping(monitor=\"val_f1\",\n",
        "                                                min_delta=1e-2,patience=5,mode='max',verbose=1,\n",
        "                                                restore_best_weights=True)\n",
        "            ]\n",
        "            history = models[feature].fit(\n",
        "                x=train[feature][0], y=train[feature][1], epochs=100,\n",
        "                validation_data=(test[feature][0],test[feature][1])\n",
        "            )\n",
        "\n",
        "            # validation score-----------------------------------------------------------------------\n",
        "            true = valid[feature][1]\n",
        "            pred = np.argmax(models[feature].predict(valid[feature][0]), axis=1)\n",
        "            cm = confusion_matrix(true, pred, c=class_num)\n",
        "            cm0 = (cm if np.sum(cm) == 0.0 else cm / np.sum(cm))\n",
        "            print(cm)\n",
        "            print(cm0)\n",
        "            f1 = f1_score(cm)\n",
        "            if f1 >= score[feature][-1]:\n",
        "                print(\n",
        "                    '-------------------------------new high f1 score: feature=%s, balance=%s, f1=%s -------------------------------\\n%s'\\\n",
        "                    %(feature,balance_w,f1,hyper))\n",
        "                score[feature] = [hyper, [list(l) for l in list(cm)], f1]\n",
        "                weight_path    = '/'.join(in_path.split('/')[:-1])+'/%s.%s.%s.weights.h5'%(feature,cell_label_str,balance_w)\n",
        "                if os.path.exists(json_score):\n",
        "                    with open(json_score,'r') as f:  old_score          = json.load(f)\n",
        "                    if feature in old_score:         old_score[feature] = score[feature]\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "                else:\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "            else:\n",
        "                print('f1 score: feature=%s, balance=%s, f1=%s'%(feature,balance_w, f1))\n",
        "\n",
        "            print({cell: len(F[feature][cell]) for cell in F[feature]})\n",
        "            min_score = max([len(F[feature][cell]) for cell in F[feature]]) / sum(\n",
        "                [len(F[feature][cell]) for cell in F[feature]])\n",
        "            print(min_score)\n",
        "            print({cell: len(C[feature][cell]) for cell in C[feature]})\n",
        "            min_score = max([len(C[feature][cell]) for cell in C[feature]]) / sum(\n",
        "                [len(C[feature][cell]) for cell in C[feature]])\n",
        "            print(min_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "130r0o1OABt7BqiCKFKPVwKS9cfZf0aKZ",
      "authorship_tag": "ABX9TyOGzVJC1e+xPrKlQfdM1PLz"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}