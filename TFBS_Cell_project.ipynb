{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-psvXpKG_x8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D9OdA0JYgif",
        "outputId": "ad9b0018-c7f6-4147-c84d-d5feff59d712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e: print(e)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NFwoVcfHYx7",
        "outputId": "b00db15a-6ed8-4b3f-b4c4-26ebb1a847e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_DIR=\"/content/drive/MyDrive/Shared/TFBS_Cell_project/\"\n",
        "in_path=ROOT_DIR+\"cellvar.db.tfbs_seq.tsv.gz\"\n",
        "out_dir=ROOT_DIR+\"out\"\n",
        "\n",
        "if path.exists(out_dir) == False:\n",
        "    os.mkdir(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "EiRPOh24YoMS"
      },
      "outputs": [],
      "source": [
        "# des = \"\"\"\n",
        "# TFBS sequence training/learning tool\n",
        "# (C) Timothy James Becker, 05/03/24-07/08/24, version=0.0.1\"\"\"\n",
        "# parser = argparse.ArgumentParser(description=des, formatter_class=argparse.RawTextHelpFormatter)\n",
        "# # need the split, max vocab, maxlen, balance, cmap,tsv.gz input file\n",
        "# parser.add_argument('--in_path', type=str, help='cellvar.db.tfbs_seq.tsv.gz traing input file')\n",
        "# parser.add_argument('--out_dir', type=str, help='output directory')\n",
        "# parser.add_argument('--features', type=str, help='comma-seperated list fo features to model: AEC,PEC,PC,IC')\n",
        "# parser.add_argument('--split', type=float, help='split factor [0.0 to 1.0]')\n",
        "# parser.add_argument('--vocab', type=int, help='maximum vocabulary size')\n",
        "# parser.add_argument('--len', type=int, help='sequence length')\n",
        "# parser.add_argument('--balance', type=float, help='balance factor [0.0 is downsample to 1.0 is upsample]')\n",
        "# parser.add_argument('--gpu', type=int, help='gpu number [0 to x]')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# in_path = args.in_path\n",
        "# features = args.features.split(',')\n",
        "# features = set([feature.upper() for feature in features]).intersection(set(['AEC', 'PEC', 'PC', 'IC']))\n",
        "# split = args.split\n",
        "# max_vocab = args.vocab\n",
        "# maxlen = args.len\n",
        "# balance_w = args.balance\n",
        "# gpu_num = args.gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YRp0xKLHDAI"
      },
      "source": [
        "## Network layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NHNmYBMAYy48"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim,self.num_heads,self.ff_dim,self.rate = embed_dim, num_heads, ff_dim, rate\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.maxlen,self.vocab_size,self.embed_dim = maxlen, vocab_size, embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z_yeyVpJY5Qa"
      },
      "outputs": [],
      "source": [
        "class F1(keras.metrics.Metric):\n",
        "    def __init__(self, name='f1'):\n",
        "        super().__init__(name=name)\n",
        "        self.sum_t0   = self.add_variable(shape=(),initializer='zeros',name='sum_t0')\n",
        "        self.sum_p0   = self.add_variable(shape=(),initializer='zeros',name='sum_p0')\n",
        "        self.sum_t0p0 = self.add_variable(shape=(),initializer='zeros',name='sum_t0p0')\n",
        "        self.sum_t1   = self.add_variable(shape=(),initializer='zeros',name='sum_t1')\n",
        "        self.sum_p1   = self.add_variable(shape=(),initializer='zeros',name='sum_p1')\n",
        "        self.sum_t1p1 = self.add_variable(shape=(),initializer='zeros',name='sum_t1p1')\n",
        "        self.f1       = self.add_variable(shape=(),initializer='zeros',name='f1')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        t0 = -1 * (y_true - 1) + 0.0\n",
        "        t1 = y_true\n",
        "        p0 = ops.round(y_pred[:, 0],0)\n",
        "        p1 = ops.round(y_pred[:, 1],0)\n",
        "        self.sum_t0.assign(self.sum_t0+ops.sum(t0))\n",
        "        self.sum_p0.assign(self.sum_p0+ops.sum(p0))\n",
        "        self.sum_t0p0.assign(self.sum_t0p0+ops.sum(t0*p0))\n",
        "        self.sum_t1.assign( self.sum_t1+ops.sum(t1))\n",
        "        self.sum_p1.assign(self.sum_p1+ops.sum(p1))\n",
        "        self.sum_t1p1.assign(self.sum_t1p1+ops.sum(t1*p1))\n",
        "\n",
        "    def result(self):\n",
        "        prec0 = ops.divide_no_nan(self.sum_t0p0,self.sum_t0)\n",
        "        rec0  = ops.divide_no_nan(self.sum_t0p0,self.sum_p0)\n",
        "        prec1 = ops.divide_no_nan(self.sum_t1p1,self.sum_t1)\n",
        "        rec1  = ops.divide_no_nan(self.sum_t1p1,self.sum_p1)\n",
        "        f1_0  = 2.0*ops.divide_no_nan(prec0*rec0,prec0+rec0)\n",
        "        f1_1  = 2.0*ops.divide_no_nan(prec1*rec1,prec1+rec1)\n",
        "        self.f1.assign((f1_0+f1_1)/2.0)\n",
        "        return self.f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSak6Pz3HIbA"
      },
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gcCFvAiTY-qK"
      },
      "outputs": [],
      "source": [
        "def c2_f1_loss(y_true, y_pred):\n",
        "    t0 = -1 * (y_true - 1) + 0.0\n",
        "    t1 = y_true\n",
        "    p0 = y_pred[:, 0]\n",
        "    p1 = y_pred[:, 1]\n",
        "    e10 = t1*p0\n",
        "    e01 = t0*p1\n",
        "    e = ops.sum(e10) + ops.sum(e01)\n",
        "    x1 = ops.sum(t1 * p0) / e\n",
        "    x2 = ops.sum(t0 * p1) / e\n",
        "    f1 = 1.0 - 2.0 * (x1 * x2) / (x1 + x2)\n",
        "    return ops.sum(f1 * e10) + ops.sum(f1 * e01)\n",
        "\n",
        "def data2dict(raw):\n",
        "    F = {}\n",
        "    for row in raw:\n",
        "        feature, cell = row[:2]\n",
        "        if feature not in F:       F[feature] = {}\n",
        "        if cell not in F[feature]: F[feature][cell] = []\n",
        "        F[feature][cell] += [row[2:]]\n",
        "    return F\n",
        "\n",
        "def split_num(s):\n",
        "    match = re.match(r\"([a-z]*)([0-9]+)([a-z]*)\", s, re.I)\n",
        "    if match:\n",
        "        items = list(match.groups())\n",
        "    else:\n",
        "        items = [s]\n",
        "    while items[-1] == '': items = items[:-1]\n",
        "    return items\n",
        "\n",
        "def conv(s):\n",
        "    ss = split_num(s)\n",
        "    t = ''\n",
        "    for x in ss:\n",
        "        if x.isdigit():\n",
        "            t += chr(int(x) + ord('z') + 1)\n",
        "        else:\n",
        "            t += x\n",
        "    return t\n",
        "\n",
        "def edit_sim(s1, s2, w=[0, 1, 1, 1]):\n",
        "    s1 = conv(s1.replace('+', '').replace('-', ''))\n",
        "    s2 = conv(s2.replace('+', '').replace('-', ''))\n",
        "    if len(s1) < len(s2): s1, s2 = s2, s1\n",
        "    u, v = len(s1), len(s2)\n",
        "    d = [[0 for col in range(v + 1)] for row in range(u + 1)]\n",
        "    for i in range(0, u + 1): d[i][0] = i\n",
        "    for j in range(0, v + 1): d[0][j] = j\n",
        "    for j in range(1, v + 1):\n",
        "        for i in range(1, u + 1):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1] + w[0]\n",
        "            else:\n",
        "                d[i][j] = min(d[i - 1][j] + w[1], d[i][j - 1] + w[2], d[i - 1][j - 1] + w[3])\n",
        "    return 1.0 - d[u][v] / max(len(s1), len(s2))\n",
        "\n",
        "# given sequences F[feature][cell] = [['RUNX2+',...],[],[],...] with len n, up-sample to target value m>n\n",
        "# up sampling should use a generative model, something like a kmer=3 distribution...\n",
        "def upsample_seqs(teqs, m, strings=True):\n",
        "    if strings:\n",
        "        seqs = [seq.split(' ') for seq in teqs]\n",
        "    else:\n",
        "        seqs = teqs\n",
        "    H = {}\n",
        "    for seq in seqs:\n",
        "        if len(seqs) >= 2:\n",
        "            for i in range(0, len(seq) - 1, 1):\n",
        "                a, b = seq[i], seq[i + 1]\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "            for i in range(len(seq) - 1, 0, -1):\n",
        "                a, b = seq[i].replace('+', '-'), seq[i - 1].replace('-', '+')\n",
        "                if a not in H:    H[a] = {}\n",
        "                if b not in H[a]:\n",
        "                    H[a][b] = 1\n",
        "                else:\n",
        "                    H[a][b] += 1\n",
        "    G, H = [], {h: {c: H[h][c] for c in sorted(H[h])} for h in sorted(H)}\n",
        "    ss = list(np.random.choice(list(H), m, replace=True))\n",
        "    for i in range(len(ss)):\n",
        "        s, g, n = ss[i], [], np.random.choice([len(seq) for seq in seqs])\n",
        "        for i in range(n):\n",
        "            g += [s]\n",
        "            if s in H:\n",
        "                s = np.random.choice(list(H[s]))\n",
        "            else:\n",
        "                s = np.random.choice(ss)\n",
        "        G += [g]\n",
        "    if strings: G = [' '.join(seq) for seq in G]\n",
        "    return G\n",
        "\n",
        "def get_cell_idx(D):\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    return cell_idx\n",
        "\n",
        "def data_partition(D, split=0.3, shuffle=True,\n",
        "                    balance_w=None):  # will take 70% for training, 20% for test and 10% for validation of each stratification, using spilt=0.3\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = {}, {}, {}, {}, {}, {}\n",
        "    cells = sorted(list(set([tuple(sorted(D[feature])) for feature in D]))[0])\n",
        "    cell_idx = {cells[i]: i for i in range(len(cells))}\n",
        "    print('using data labeling scheme: %s'%cell_idx)\n",
        "    for feature in D:\n",
        "        train_x[feature], train_y[feature] = {}, {}\n",
        "        test_x[feature], test_y[feature] = {}, {}\n",
        "        valid_x[feature], valid_y[feature] = {}, {}\n",
        "        for cell in sorted(D[feature]):\n",
        "            if cell in cell_idx:\n",
        "                n = len(D[feature][cell])\n",
        "                u = int(round(n * (1.0 - split)))\n",
        "                v = int(round(2 * n * split / 3.0))\n",
        "                u_idx = np.random.choice(range(n), u, replace=False)\n",
        "                v_idx = np.random.choice(list(set(range(n)).difference(set(u_idx))), v, replace=False)\n",
        "                w_idx = set(range(n)).difference(set(u_idx).union(set(v_idx)))\n",
        "                train_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in u_idx]\n",
        "                test_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in v_idx]\n",
        "                valid_x[feature][cell] = [' '.join(D[feature][cell][i]) for i in w_idx]\n",
        "\n",
        "    if balance_w is not None:  # balance the data using a variable 0.0-1.0 up/down sampling point\n",
        "        print('balancing data...')\n",
        "        for feature in train_x:\n",
        "            min_class = min([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            max_class = max([len(train_x[feature][cell]) for cell in train_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(train_x[feature]):\n",
        "                print('balancing training feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(train_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    train_x[feature][cell] = [train_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    train_x[feature][cell] += upsample_seqs(train_x[feature][cell], mid_class - fc_len,\n",
        "                                                            strings=True)\n",
        "        for feature in test_x:\n",
        "            min_class = min([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            max_class = max([len(test_x[feature][cell]) for cell in test_x[feature]])\n",
        "            mid_class = int(round(min_class + balance_w * (max_class - min_class)))\n",
        "            for cell in sorted(test_x[feature]):\n",
        "                print('balancing test feature=%s, cell=%s' % (feature, cell))\n",
        "                fc_len = len(test_x[feature][cell])\n",
        "                if fc_len >= mid_class:\n",
        "                    test_x[feature][cell] = [test_x[feature][cell][i] for i in\n",
        "                                                np.random.choice(range(fc_len), mid_class, replace=False)]\n",
        "                else:\n",
        "                    test_x[feature][cell] += upsample_seqs(test_x[feature][cell], mid_class - fc_len, strings=True)\n",
        "\n",
        "    for feature in train_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(train_x[feature]):\n",
        "            X += train_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(train_x[feature][cell]))]\n",
        "        train_x[feature], train_y[feature] = X, Y\n",
        "    for feature in test_x:\n",
        "        X, Y = [], []\n",
        "        for cell in sorted(test_x[feature]):\n",
        "            X += test_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(test_x[feature][cell]))]\n",
        "        test_x[feature], test_y[feature] = X, Y\n",
        "        X, Y = [], []\n",
        "    for feature in valid_x:\n",
        "        for cell in sorted(valid_x[feature]):\n",
        "            X += valid_x[feature][cell]\n",
        "            Y += [cell_idx[cell] for i in range(len(valid_x[feature][cell]))]\n",
        "        valid_x[feature], valid_y[feature] = X, Y\n",
        "\n",
        "    if shuffle:  # have the correct number of examples but the labels are not shuffled\n",
        "        for feature in D:\n",
        "            n = len(train_x[feature])\n",
        "            idx = np.random.choice(range(n), n, replace=False)\n",
        "            train_x[feature] = [train_x[feature][i] for i in idx]\n",
        "            train_y[feature] = [train_y[feature][i] for i in idx]\n",
        "\n",
        "            u = len(test_x[feature])\n",
        "            idx = np.random.choice(range(u), u, replace=False)\n",
        "            test_x[feature] = [test_x[feature][i] for i in idx]\n",
        "            test_y[feature] = [test_y[feature][i] for i in idx]\n",
        "\n",
        "            v = len(valid_x[feature])\n",
        "            idx = np.random.choice(range(v), v, replace=False)\n",
        "            valid_x[feature] = [valid_x[feature][i] for i in idx]\n",
        "            valid_y[feature] = [valid_y[feature][i] for i in idx]\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
        "\n",
        "def preprocess_data(D, max_vocab=None, maxlen=2000, split=0.4, balance_w=0.5, dt=np.float32):\n",
        "    train_x, train_y, test_x, test_y, valid_x, valid_y = data_partition(D, split=split, balance_w=balance_w)\n",
        "    vocab = sorted(get_vocab(D))\n",
        "    vocab_size = len(vocab)\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=\"lower\",\n",
        "        max_tokens=min(vocab_size, max_vocab),\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=maxlen\n",
        "    )\n",
        "    vectorize_layer.adapt(vocab)\n",
        "    train, test, valid = {}, {}, {}\n",
        "    for feature in D:\n",
        "        train[feature] = (\n",
        "        np.array(vectorize_layer(train_x[feature]), dtype=dt), np.array(train_y[feature], dtype=dt))\n",
        "        test[feature] = (np.array(vectorize_layer(test_x[feature]), dtype=dt), np.array(test_y[feature], dtype=dt))\n",
        "        valid[feature] = (\n",
        "        np.array(vectorize_layer(valid_x[feature]), dtype=dt), np.array(valid_y[feature], dtype=dt))\n",
        "    return train, test, valid, vectorize_layer\n",
        "\n",
        "def get_vocab(D):\n",
        "    V = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            for row in D[feature][cell]:\n",
        "                for i in row:\n",
        "                    V.add(i)\n",
        "    return V\n",
        "\n",
        "def get_trans(C, ks=16):\n",
        "    S = {}\n",
        "    V = sorted(set([re.split('[+|-]', v)[0] for v in get_vocab(C)]))\n",
        "    idx = {V[i]: i for i in range(len(V))}\n",
        "    for feature in C:\n",
        "        S[feature] = {}\n",
        "        for cell in C[feature]:\n",
        "            S[feature][cell] = []\n",
        "            for i in range(len(C[feature][cell])):\n",
        "                X = np.zeros((len(idx), len(idx), 3), dtype=float)\n",
        "                for k in range(min(ks, len(C[feature][cell][i]))):\n",
        "                    for a in range(len(C[feature][cell][i]) - k):\n",
        "                        tf1 = re.split('[+|-]', C[feature][cell][i][a])[0]\n",
        "                        tf2 = re.split('[+|-]', C[feature][cell][i][a + k])[0]\n",
        "                        if C[feature][cell][i][a].find('+') > -1:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                        else:\n",
        "                            if C[feature][cell][i][a + k].find('+') > -1:\n",
        "                                X[idx[tf1], idx[tf2], 1] += 1\n",
        "                            else:\n",
        "                                X[idx[tf1], idx[tf2], 0] += 1\n",
        "                X /= np.sum(X)\n",
        "                S[feature][cell] += [X]\n",
        "    return S\n",
        "\n",
        "def get_classes(D):\n",
        "    C = set([])\n",
        "    for feature in D:\n",
        "        for cell in D[feature]:\n",
        "            C.add(cell)\n",
        "    return C\n",
        "\n",
        "def cell_map(F, cmap):\n",
        "    C = {}\n",
        "    for feature in F:\n",
        "        C[feature] = {}\n",
        "        for cell in F[feature]:\n",
        "            if cmap[cell] in C[feature]:\n",
        "                C[feature][cmap[cell]] += [row for row in F[feature][cell]]\n",
        "            else:\n",
        "                C[feature][cmap[cell]] = [row for row in F[feature][cell]]\n",
        "    return C\n",
        "\n",
        "def confusion_matrix(T, Y, c=10):\n",
        "    M = np.array([[0.0 for j in range(c)] for i in range(c)], dtype=float)\n",
        "    for i in range(len(T)):\n",
        "        M[int(T[i])][int(Y[i])] += 1.0\n",
        "    return M\n",
        "\n",
        "def f1_score(cm):\n",
        "    f1s = []\n",
        "    for i in range(len(cm)):\n",
        "        prec = (0.0 if np.sum(cm[:, i]) == 0.0 else cm[i, i] / np.sum(cm[:, i]))\n",
        "        rec = (0.0 if np.sum(cm[i, :]) == 0.0 else cm[i, i] / np.sum(cm[i, :]))\n",
        "        f1s += [(0.0 if (prec + rec) == 0.0 else 2 * (prec * rec) / (prec + rec))]\n",
        "    return (0.0 if () == 0.0 else len(cm) * (np.prod(f1s)) / (np.sum(f1s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvPDUKD8HM2v"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RLZWEFAoJvJ"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "58aFffPZIVLA"
      },
      "outputs": [],
      "source": [
        "# comma-seperated list fo features to model: AEC,PEC,PC,IC\n",
        "features=[\"AEC\"]\n",
        "\n",
        "# split factor [0.0 to 1.0]\n",
        "split=0.3\n",
        "\n",
        "# maximum vocabulary size\n",
        "max_vocab=500\n",
        "\n",
        "# sequence length\n",
        "maxlen=50\n",
        "\n",
        "# balance factor [0.0 is downsample to 1.0 is upsample]\n",
        "balance_w=None\n",
        "\n",
        "# gpu number [0 to x]\n",
        "gpu_num=0\n",
        "\n",
        "# number of epochs for training\n",
        "epochs=25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nh-Zyc_JKDEs"
      },
      "outputs": [],
      "source": [
        "# read in the raw data sequences as: feature,cell,x1,x2,...xm\n",
        "with gzip.GzipFile(in_path, 'rb') as f:\n",
        "    raw = [row.decode('utf-8').replace('\\n', '').split('\\t') for row in f.readlines()]\n",
        "F = data2dict(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFWIbNQUK_SY",
        "outputId": "bb78898b-6b02-47d9-f2af-e5fc41059972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AEC', 'IC', 'PEC', 'PC']\n",
            "['MESOTHELIALEPICARDIUM', 'SPLEEN', 'STOMACH', 'HEPATOCYTE', 'NEURALCREST', 'NEPHRONPROGENITOR', 'OSTEOCYTE', 'PANCREAS', 'MESENCHYMAL', 'PERIPHERALBLOOD']\n"
          ]
        }
      ],
      "source": [
        "print(list(F))\n",
        "print(list(F['AEC']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t2HUB7PWoGDC"
      },
      "outputs": [],
      "source": [
        "# apply cell map to F\n",
        "cmap = {'MESENCHYMAL': 'OFC', 'NEURALCREST': 'OFC', 'OSTEOCYTE': 'BACKGROUND',\n",
        "        'HEPATOCYTE': 'BACKGROUND', 'MESOTHELIALEPICARDIUM': 'BACKGROUND', 'NEPHRONPROGENITOR': 'BACKGROUND',\n",
        "        'PANCREAS': 'BACKGROUND', 'PERIPHERALBLOOD': 'BACKGROUND', 'SPLEEN': 'BACKGROUND', 'STOMACH': 'BACKGROUND'}\n",
        "# cmap = {k:k for k in set([c for f in F for c in F[f]])}\n",
        "C = cell_map(F, cmap)\n",
        "C = {feature: C[feature] for feature in features}  # apply only features that are selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWwfnmlSoDGy",
        "outputId": "21244ae9-b526-4fd3-f2d2-82431abe00a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BACKGROUND', 'OFC']\n"
          ]
        }
      ],
      "source": [
        "print(list(C['AEC']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "raw = C['AEC']['OFC']\n",
        "data = [0] * len(raw)\n",
        "for i in range(len(raw)):\n",
        "    data[i] = len(raw[i])\n",
        "print(statistics.median_high(data))\n",
        "\n",
        "max_len = 0\n",
        "for d in data:\n",
        "    max_len = max(max_len, d)\n",
        "print(max_len)\n",
        "\n",
        "vocab = set()\n",
        "for seq in raw:\n",
        "    for w in seq:\n",
        "        if w not in vocab:\n",
        "            vocab.add(w)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbvHdd5iMbE_",
        "outputId": "5cf89e1e-5443-4f14-ed1c-153c0a6e9516"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "194\n",
            "1111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6XiH_hwoMpd"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8u0vGiSuoQMn"
      },
      "outputs": [],
      "source": [
        "#embed_dims,num_heads,ff_dims,batches,drops = [4],[2],[16],[128],[0.3]\n",
        "embed_dims,num_heads,ff_dims,batches,drops = [2,4,8],[2,4,8],[16,32,64],[32,64,128],[0.4,0.5]\n",
        "hyper_params = []\n",
        "for embed_dim in embed_dims:\n",
        "    for num_head in num_heads:\n",
        "        for ff_dim in ff_dims:\n",
        "            for batch in batches:\n",
        "                for drop in drops:\n",
        "                    hyper_params += [[embed_dim, num_head, ff_dim, batch, drop, balance_w]]\n",
        "\n",
        "class_num = len(get_classes(C))\n",
        "vocab_size = len(sorted(get_vocab(C)))\n",
        "max_vocab = min(max_vocab, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8DuB9-xoSMR",
        "outputId": "8820fcf6-61b7-40e7-ea49-37354daa030c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing data now...\n",
            "using data labeling scheme: {'BACKGROUND': 0, 'OFC': 1}\n"
          ]
        }
      ],
      "source": [
        "print('preprocessing data now...')\n",
        "train, test, valid, vec = preprocess_data(C, max_vocab=max_vocab, maxlen=maxlen, split=split, balance_w=balance_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nslYv2koTjK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "cell_idx = get_cell_idx(C)\n",
        "cell_label_str = '_'.join([k+'-%s'%cell_idx[k] for k in sorted(cell_idx)])\n",
        "json_score = out_dir+'/%s.%s.%s.score.json'%('_'.join(sorted(features)),cell_label_str,balance_w)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%y%m%d%H%M\")\n",
        "jsonl_timestamp = out_dir+'/timestamp.%s.%s.%s.score.jsonl'%('_'.join(sorted(features)),cell_label_str,balance_w)\n",
        "\n",
        "if os.path.exists(json_score):\n",
        "    with open(json_score,'r') as f: score = json.load(f)\n",
        "else: score = {feature: [hyper_params[0],[], 0.0] for feature in C}"
      ],
      "metadata": {
        "id": "3JPpkA0Y4BEv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad9wHZ06ZF9e",
        "outputId": "5f0450a1-d233-4907-ebfc-8e77e458eeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 4ms/step - f1: 0.4513 - loss: 6.0115 - val_f1: 0.4430 - val_loss: 4.9830\n",
            "Epoch 2/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3ms/step - f1: 0.4433 - loss: 4.9644 - val_f1: 0.4430 - val_loss: 5.0011\n",
            "Epoch 3/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - f1: 0.4429 - loss: 5.0033 - val_f1: 0.4430 - val_loss: 4.9934\n",
            "Epoch 4/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - f1: 0.4431 - loss: 4.9644 - val_f1: 0.4430 - val_loss: 4.9906\n",
            "Epoch 5/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - f1: 0.4428 - loss: 4.9913 - val_f1: 0.4430 - val_loss: 4.9730\n",
            "Epoch 6/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3ms/step - f1: 0.4429 - loss: 4.9915 - val_f1: 0.4430 - val_loss: 4.9786\n",
            "Epoch 7/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4431 - loss: 4.9440 - val_f1: 0.4430 - val_loss: 4.9547\n",
            "Epoch 8/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - f1: 0.4429 - loss: 4.9162 - val_f1: 0.4430 - val_loss: 5.0086\n",
            "Epoch 9/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - f1: 0.4432 - loss: 4.8946 - val_f1: 0.4430 - val_loss: 4.8880\n",
            "Epoch 10/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - f1: 0.4431 - loss: 4.8923 - val_f1: 0.4430 - val_loss: 4.9192\n",
            "Epoch 11/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - f1: 0.4432 - loss: 4.9183 - val_f1: 0.4430 - val_loss: 4.9146\n",
            "Epoch 12/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4432 - loss: 4.8965 - val_f1: 0.4430 - val_loss: 4.8874\n",
            "Epoch 13/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - f1: 0.4434 - loss: 4.8550 - val_f1: 0.4430 - val_loss: 4.9132\n",
            "Epoch 14/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - f1: 0.4430 - loss: 4.8917 - val_f1: 0.4430 - val_loss: 4.9217\n",
            "Epoch 15/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - f1: 0.4430 - loss: 4.9041 - val_f1: 0.4430 - val_loss: 4.8968\n",
            "Epoch 16/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - f1: 0.4424 - loss: 4.9234 - val_f1: 0.4430 - val_loss: 4.8840\n",
            "Epoch 17/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - f1: 0.4430 - loss: 4.9003 - val_f1: 0.4430 - val_loss: 4.8981\n",
            "Epoch 18/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4425 - loss: 4.9357 - val_f1: 0.4430 - val_loss: 4.8957\n",
            "Epoch 19/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - f1: 0.4433 - loss: 4.8813 - val_f1: 0.4430 - val_loss: 4.9073\n",
            "Epoch 20/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4432 - loss: 4.9192 - val_f1: 0.4430 - val_loss: 4.8967\n",
            "Epoch 21/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4431 - loss: 4.8735 - val_f1: 0.4430 - val_loss: 4.9049\n",
            "Epoch 22/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - f1: 0.4432 - loss: 4.9161 - val_f1: 0.4430 - val_loss: 4.8847\n",
            "Epoch 23/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - f1: 0.4428 - loss: 4.8947 - val_f1: 0.4430 - val_loss: 4.9104\n",
            "Epoch 24/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - f1: 0.4429 - loss: 4.8829 - val_f1: 0.4430 - val_loss: 4.8834\n",
            "Epoch 25/25\n",
            "\u001b[1m8572/8572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - f1: 0.4428 - loss: 4.9113 - val_f1: 0.4430 - val_loss: 4.9024\n",
            "\u001b[1m1225/1225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
            "[[31162.     0.]\n",
            " [ 8021.     0.]]\n",
            "[[0.79529388 0.        ]\n",
            " [0.20470612 0.        ]]\n",
            "-------------------------------new high f1 score: feature=AEC, balance=None, f1=0.0 -------------------------------\n",
            "[2, 2, 16, 32, 0.4, None]\n",
            "{'MESOTHELIALEPICARDIUM': 49001, 'SPLEEN': 19861, 'STOMACH': 29039, 'HEPATOCYTE': 38539, 'NEURALCREST': 63538, 'NEPHRONPROGENITOR': 108920, 'OSTEOCYTE': 52642, 'PANCREAS': 13408, 'MESENCHYMAL': 16669, 'PERIPHERALBLOOD': 211}\n",
            "0.2779791132844003\n",
            "{'BACKGROUND': 311621, 'OFC': 80207}\n",
            "0.7953004889900671\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2be2885eedb4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                 restore_best_weights=True)\n\u001b[1;32m     28\u001b[0m             ]\n\u001b[0;32m---> 29\u001b[0;31m             history = models[feature].fit(\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "models = {}\n",
        "for feature in C:\n",
        "    for hyper in hyper_params:\n",
        "        embed_dim, num_head, ff_dim, batch, drop, bl = hyper\n",
        "        with tf.device('/device:GPU:%s'%gpu_num):\n",
        "            # -----------------------------------------------------------------------\n",
        "            inputs = layers.Input(shape=(maxlen,))\n",
        "            embedding_layer = TokenAndPositionEmbedding(maxlen, max_vocab, embed_dim)\n",
        "            x = embedding_layer(inputs)\n",
        "            transformer_block = TransformerBlock(embed_dim, num_head, ff_dim)\n",
        "            x = transformer_block(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "            x = layers.Dropout(0.2)(x)\n",
        "            x = layers.Dense(class_num * 64, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(drop)(x)\n",
        "            outputs = layers.Dense(class_num, activation=\"softmax\")(x)\n",
        "            models[feature] = keras.Model(inputs=inputs, outputs=outputs)\n",
        "            models[feature].compile(optimizer=keras.optimizers.Adam(5e-5),\n",
        "                                    loss=c2_f1_loss,\n",
        "                                    metrics=[F1()])\n",
        "            models[feature].name = feature\n",
        "\n",
        "            # training-----------------------------------------------------------------------\n",
        "            callbacks = [\n",
        "                keras.callbacks.EarlyStopping(monitor=\"val_f1\",\n",
        "                                                min_delta=1e-2,patience=5,mode='max',verbose=1,\n",
        "                                                restore_best_weights=True)\n",
        "            ]\n",
        "            history = models[feature].fit(\n",
        "                x=train[feature][0], y=train[feature][1], epochs=epochs,\n",
        "                validation_data=(test[feature][0],test[feature][1])\n",
        "            )\n",
        "\n",
        "            # validation score-----------------------------------------------------------------------\n",
        "            true = valid[feature][1]\n",
        "            pred = np.argmax(models[feature].predict(valid[feature][0]), axis=1)\n",
        "            cm = confusion_matrix(true, pred, c=class_num)\n",
        "            cm0 = (cm if np.sum(cm) == 0.0 else cm / np.sum(cm))\n",
        "            print(cm)\n",
        "            print(cm0)\n",
        "            f1 = f1_score(cm)\n",
        "            if f1 >= score[feature][-1]:\n",
        "                print(\n",
        "                    '-------------------------------new high f1 score: feature=%s, balance=%s, f1=%s -------------------------------\\n%s'\\\n",
        "                    %(feature,balance_w,f1,hyper))\n",
        "                score[feature] = [hyper, [list(l) for l in list(cm)], f1]\n",
        "                weight_path    = '/'.join(in_path.split('/')[:-1])+'/%s.%s.%s.weights.h5'%(feature,cell_label_str,balance_w)\n",
        "                if os.path.exists(json_score):\n",
        "                    with open(json_score,'r') as f:  old_score          = json.load(f)\n",
        "                    if feature in old_score:         old_score[feature] = score[feature]\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "                else:\n",
        "                    with open(json_score,'w') as f:\n",
        "                        json.dump(score,f)\n",
        "                        models[feature].save_weights(weight_path)\n",
        "                # JSONL format with timestamp\n",
        "                if os.path.exists(jsonl_timestamp):\n",
        "                    score[\"timestamp\"] = timestamp\n",
        "                    with open(jsonl_timestamp, 'a') as f:\n",
        "                        f.write(json.dumps(score) + '\\n')\n",
        "                else:\n",
        "                    with open(jsonl_timestamp,'w') as f:\n",
        "                        f.write(json.dumps(score) + '\\n')\n",
        "            else:\n",
        "                print('f1 score: feature=%s, balance=%s, f1=%s'%(feature,balance_w, f1))\n",
        "\n",
        "            print({cell: len(F[feature][cell]) for cell in F[feature]})\n",
        "            min_score = max([len(F[feature][cell]) for cell in F[feature]]) / sum(\n",
        "                [len(F[feature][cell]) for cell in F[feature]])\n",
        "            print(min_score)\n",
        "            print({cell: len(C[feature][cell]) for cell in C[feature]})\n",
        "            min_score = max([len(C[feature][cell]) for cell in C[feature]]) / sum(\n",
        "                [len(C[feature][cell]) for cell in C[feature]])\n",
        "            print(min_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "u-psvXpKG_x8",
        "9YRp0xKLHDAI"
      ],
      "mount_file_id": "130r0o1OABt7BqiCKFKPVwKS9cfZf0aKZ",
      "authorship_tag": "ABX9TyPWlK2qfE33uW2wX6mLIu5I"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}